{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "c1458e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08bc65b",
   "metadata": {},
   "source": [
    "## Eliminate Implication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "3f63ead7-d2a6-46d5-a8e1-b0abba52b4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def left_sentence(expression, index):\n",
    "    # Initialize parentheses counter\n",
    "    parentheses_count = 0\n",
    "    \n",
    "    # Iterate backward from the given index\n",
    "    while index >= 0:\n",
    "        if expression[index] == ')':\n",
    "            # Increment counter for closing parenthesis\n",
    "            parentheses_count += 1\n",
    "        elif expression[index] == '(':\n",
    "            # Decrement counter for opening parenthesis\n",
    "            parentheses_count -= 1\n",
    "            # Check if matching parenthesis for current parenthesis is found\n",
    "            if parentheses_count == -1:\n",
    "                # If the previous 2 characters are quantifiers, include them in the left sentence\n",
    "                if index >= 2 and expression[index - 2:index] in ['∀', '∃']:\n",
    "                    index -= 2\n",
    "                break\n",
    "        # If no parentheses are open and the character is a logical connector or the beginning of the expression\n",
    "        elif parentheses_count == 0 and (index == 0 or expression[index - 1] in ['∧', '∨', '(', '∀', '∃']):\n",
    "            break\n",
    "        # Move index backwards\n",
    "        index -= 1\n",
    "    return index\n",
    "\n",
    "\n",
    "def right_sentence(expression, index):\n",
    "    # Initialize parentheses counter\n",
    "    parentheses_count = 0\n",
    "    \n",
    "    # Iterate forward from the given index\n",
    "    while index < len(expression):\n",
    "        if expression[index] == '(':\n",
    "            # Increment counter for opening parenthesis\n",
    "            parentheses_count += 1\n",
    "        elif expression[index] == ')':\n",
    "            # Decrement counter for closing parenthesis\n",
    "            parentheses_count -= 1\n",
    "        # If no parentheses are open and the character is a logical connector\n",
    "        elif parentheses_count == 0 and expression[index] in ['∧', '∨']:\n",
    "            # Move index backwards to include the logical connector\n",
    "            index -= 1\n",
    "            break\n",
    "        # Move index forward\n",
    "        index += 1\n",
    "    return index\n",
    "\n",
    "\n",
    "def eliminate_implication(expression):\n",
    "    index = 0\n",
    "    # Iterate through the expression\n",
    "    while index < len(expression):\n",
    "        if expression[index:index + 2] == '->':\n",
    "            # Find the left and right sentences around the implication\n",
    "            left_sentence_index = left_sentence(expression, index - 1)\n",
    "            right_sentence_index = right_sentence(expression, index + 1)\n",
    "\n",
    "            # Extract left and right sentences\n",
    "            left_expression = expression[left_sentence_index:index].strip()\n",
    "            right_expression = expression[index + 2:right_sentence_index + 1].strip()\n",
    "\n",
    "            # Transform implication into its equivalent form\n",
    "            transformed_expression = f\"¬{left_expression} ∨ {right_expression}\"\n",
    "            # Replace implication with its equivalent form in the expression\n",
    "            expression = expression[:left_sentence_index] + transformed_expression + expression[right_sentence_index + 1:]\n",
    "\n",
    "            # Update index considering the length change due to transformation\n",
    "            index = left_sentence_index + len(transformed_expression) - (right_sentence_index - left_sentence_index + 1)\n",
    "        else:\n",
    "            # Move index forward if no implication is found\n",
    "            index += 1\n",
    "    return expression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa4d364",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## Move Inward Negation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "dd940014-1a75-48e0-888e-16fd16a4dbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inward_negation(expression):\n",
    "    # Remove double negations from the expression.\n",
    "    expression = another_remove_double_negations(expression)\n",
    "    # Move negations inside quantifiers such as universal (∀) and existential (∃) quantifiers.\n",
    "    expression = move_negation_inside_quantifiers(expression)\n",
    "    # Apply De Morgan's laws to distribute negations over logical connectives (∧ and ∨).\n",
    "    expression = apply_de_morgans(expression)\n",
    "    # Return the modified expression.\n",
    "    return expression\n",
    "\n",
    "# This function removes instances of double negations in the expression.\n",
    "def another_remove_double_negations(expression):\n",
    "    return expression.replace(\"¬(¬\", \"(\")\n",
    "\n",
    "# This function moves negations inside quantifiers (∀ and ∃) in the expression.\n",
    "def move_negation_inside_quantifiers(expression):\n",
    "    result = \"\"\n",
    "    i = 0\n",
    "    while i < len(expression):\n",
    "        if expression.startswith(\"¬∀\", i) or expression.startswith(\"¬∃\", i):\n",
    "            # If a negation (∀ or ∃) is encountered, it moves inside the quantifier.\n",
    "            quantifier = '∃' if expression[i+1] == '∀' else '∀'\n",
    "            result += quantifier\n",
    "            i += 2\n",
    "            var = expression[i]\n",
    "            result += var + \"¬\"\n",
    "            i += 1\n",
    "        else:\n",
    "            # Otherwise, keep the character as it is.\n",
    "            result += expression[i]\n",
    "            i += 1\n",
    "    return result\n",
    "\n",
    "# This function applies De Morgan's laws to distribute negations over logical connectives (∧ and ∨) in the expression.\n",
    "def apply_de_morgans(expression):\n",
    "    while '¬(' in expression:\n",
    "        start_index = expression.find('¬(') + 2\n",
    "        balance = 1\n",
    "        i = start_index\n",
    "        while i < len(expression) and balance != 0:\n",
    "            if expression[i] == '(':\n",
    "                balance += 1\n",
    "            elif expression[i] == ')':\n",
    "                balance -= 1\n",
    "            i += 1\n",
    "        end_index = i\n",
    "        inner_expression = expression[start_index:end_index - 1]\n",
    "\n",
    "        replaced = \"\"\n",
    "        if '∧' in inner_expression:\n",
    "            # If the inner expression contains ∧, De Morgan's law for ∧ is applied.\n",
    "            terms = inner_expression.split('∧')\n",
    "            replaced_terms = [\"¬\" + term for term in terms]\n",
    "            replaced = \" ∨ \".join(replaced_terms)\n",
    "        elif '∨' in inner_expression:\n",
    "            # If the inner expression contains ∨, De Morgan's law for ∨ is applied.\n",
    "            terms = inner_expression.split('∨')\n",
    "            replaced_terms = [\"¬\" + term for term in terms]\n",
    "            replaced = \" ∧ \".join(replaced_terms)\n",
    "        else:\n",
    "            # If the inner expression is a single variable or negated variable, negate it.\n",
    "            if inner_expression.startswith(\"¬\"):\n",
    "                replaced = inner_expression[1:]\n",
    "            else:\n",
    "                replaced = \"¬\" + inner_expression\n",
    "\n",
    "        # Replace the original expression with the modified one.\n",
    "        expression = expression[:start_index - 2] + replaced + expression[end_index:]\n",
    "\n",
    "    return expression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81d478a",
   "metadata": {},
   "source": [
    "## Remove Double Negation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "19571bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_double_negation(sentence):\n",
    "    return sentence.replace('¬¬', '')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03af0866",
   "metadata": {},
   "source": [
    "## Standardize Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "54f2f8c5-6706-44e1-a431-e668b28cdd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(expressions_string):\n",
    "    # Splitting the input string into individual expressions based on the delimiter\n",
    "    expressions = expressions_string.split(\"', '\")\n",
    "\n",
    "    standardized_expressions = []\n",
    "\n",
    "    # Iterate over each expression\n",
    "    for expression in expressions:\n",
    "        # Standardize the expression and append it to the list of standardized expressions\n",
    "        new_expression = standardize_expression(expression)\n",
    "        standardized_expressions.append(new_expression)\n",
    "\n",
    "    # Joining the standardized expressions into a single string with the original delimiter.\n",
    "    return \"', '\".join(standardized_expressions)\n",
    "\n",
    "\n",
    "def standardize_expression(expression):\n",
    "    # Set to keep track of used variables\n",
    "    used_variables = set()\n",
    "    # Dictionary to store variable replacements\n",
    "    replacement_map = {}\n",
    "    # Flags to track the first quantifier found and its variable and replacement\n",
    "    first_quantifier_found = False\n",
    "    first_quantifier_var = None\n",
    "    first_quantifier_replacement = None\n",
    "\n",
    "    # Function to generate a new variable based on the old variable\n",
    "    def generate_new_variable(old_var):\n",
    "        # If a replacement for the old variable already exists, return it\n",
    "        if old_var in replacement_map:\n",
    "            return replacement_map[old_var]\n",
    "        # Generate a new variable name\n",
    "        new_var = chr(ord('a') + len(used_variables))\n",
    "        # Ensure uniqueness of the new variable name\n",
    "        while new_var in used_variables:\n",
    "            new_var = chr(ord(new_var) + 1)\n",
    "        # Add the new variable to the set of used variables and update the replacement map\n",
    "        used_variables.add(new_var)\n",
    "        replacement_map[old_var] = new_var\n",
    "        return new_var\n",
    "\n",
    "    # Function to process quantifiers in the expression\n",
    "    def process_quantifiers(expr):\n",
    "        i = 0\n",
    "        # Iterate over the characters in the expression\n",
    "        while i < len(expr):\n",
    "            # Check if the character is a quantifier\n",
    "            if expr[i] in \"∀∃\":\n",
    "                quantifier = expr[i]\n",
    "                var = expr[i+1]\n",
    "                # Confirm it's a variable\n",
    "                if var.islower():\n",
    "                    # Check if it's the first quantifier found\n",
    "                    nonlocal first_quantifier_found, first_quantifier_var, first_quantifier_replacement\n",
    "                    if not first_quantifier_found:\n",
    "                        # Update the flag and store the first quantifier's variable and replacement\n",
    "                        first_quantifier_found = True\n",
    "                        first_quantifier_var = var\n",
    "                        first_quantifier_replacement = var\n",
    "                    else:\n",
    "                        # Generate a new variable and replace occurrences of the old variable with it\n",
    "                        new_var = generate_new_variable(var)\n",
    "                        expr = expr[:i+1] + expr[i+1:].replace(var, new_var)\n",
    "                        # Update the index considering the length change due to variable replacement\n",
    "                        i += len(new_var) - 1\n",
    "            # Move to the next character\n",
    "            i += 1\n",
    "        return expr\n",
    "\n",
    "    # Call the function to process quantifiers in the expression\n",
    "    return process_quantifiers(expression) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5df5fc",
   "metadata": {},
   "source": [
    "### Move Quantifiers to Left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "f08a778c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_quantifiers(sentence):\n",
    "    # Initialize variables to store quantified variables and the rest of the formula\n",
    "    quantified_vars = []  # List to store tuples of quantifier and associated variable\n",
    "    formula = []  # List to store the remaining formula\n",
    "\n",
    "    # Iterate through the characters of the sentence\n",
    "    i = 0\n",
    "    while i < len(sentence):\n",
    "        char = sentence[i]\n",
    "        # Check if the character is a quantifier\n",
    "        if char in ['∀', '∃']:\n",
    "            # If quantifier, find the variable associated with it\n",
    "            j = i + 1\n",
    "            var = \"\"\n",
    "            while j < len(sentence) and sentence[j].isalnum():\n",
    "                var += sentence[j]\n",
    "                j += 1\n",
    "            # Add the quantifier and its associated variable to the list\n",
    "            quantified_vars.append((char, var))\n",
    "            i = j\n",
    "        else:\n",
    "            # If not quantifier, add it to the formula\n",
    "            formula.append(char)\n",
    "            i += 1\n",
    "\n",
    "    # Construct the new sentence by moving quantifiers to the left\n",
    "    new_sentence = \"\"\n",
    "    for quantifier, var in quantified_vars:\n",
    "        new_sentence += quantifier + var\n",
    "    new_sentence += ''.join(formula)\n",
    "\n",
    "    return new_sentence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7cf690",
   "metadata": {},
   "source": [
    "## Skolemization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "69572588-2101-4034-8960-62c6d5748791",
   "metadata": {},
   "outputs": [],
   "source": [
    "def skolem(expression, universal_variables=None, skolem_functions=None, used_variables=None):\n",
    "    # Initialize variables if not provided\n",
    "    if universal_variables is None:\n",
    "        universal_variables = []\n",
    "    if skolem_functions is None:\n",
    "        skolem_functions = {}\n",
    "    if used_variables is None:\n",
    "        used_variables = set()  # Keep track of universal variables used in Skolem functions\n",
    "\n",
    "    # Nested function to handle quantifiers and variable replacements\n",
    "    def process_quantifiers(expr):\n",
    "        new_expr = \"\"\n",
    "        default_for_skolem = None  # Default value for Skolem functions\n",
    "        i = 0\n",
    "        while i < len(expr):\n",
    "            if expr[i] in [\"∀\", \"∃\"]:\n",
    "                # Handle universal (∀) and existential (∃) quantifiers\n",
    "                quantifier = expr[i]\n",
    "                variable = expr[i + 1]\n",
    "                if quantifier == \"∀\":\n",
    "                    # For universal quantifiers, add the variable to the list of universal variables\n",
    "                    universal_variables.append(variable)\n",
    "                    if default_for_skolem is None:\n",
    "                        # Update the default_for_skolem if not already set\n",
    "                        default_for_skolem = variable\n",
    "                    # Include ∀ quantifiers and variables in the new formula\n",
    "                    new_expr += f\"{quantifier}{variable} \"\n",
    "                else:  # quantifier == \"∃\"\n",
    "                    if universal_variables:\n",
    "                        # Use the first unused universal variable for the Skolem function, if available\n",
    "                        for u_variable in universal_variables:\n",
    "                            if u_variable not in used_variables:\n",
    "                                skolem_term = f\"f({u_variable})\"\n",
    "                                used_variables.add(u_variable)\n",
    "                                break\n",
    "                        else:\n",
    "                            # Use the default value if all universal variables are used\n",
    "                            skolem_term = f\"f({default_for_skolem})\"\n",
    "                    else:\n",
    "                        # Use a generic constant when no universal variables are in scope\n",
    "                        skolem_term = \"c\"  \n",
    "                    skolem_functions[variable] = skolem_term\n",
    "                i += 2  # Skip past the quantifier and variable\n",
    "                continue\n",
    "            elif expr[i] in skolem_functions:\n",
    "                # Replace existential variables with their corresponding Skolem terms\n",
    "                variable = expr[i]\n",
    "                skolem_term = skolem_functions[variable]\n",
    "                new_expr += skolem_term\n",
    "                i += 1  # Adjust the index after replacement\n",
    "            else:\n",
    "                # Directly add other characters to the new formula\n",
    "                new_expr += expr[i]\n",
    "                i += 1\n",
    "        return new_expr\n",
    "\n",
    "    # Call the nested function to process quantifiers and variable replacements in the expression\n",
    "    return process_quantifiers(expression)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413b0656",
   "metadata": {},
   "source": [
    "## Eliminate Universal Quantifers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "1557617e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def eliminate_universal_quantifiers(expression):\n",
    "    return re.sub(r'∀\\w+\\s', '', expression)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46cd68f3",
   "metadata": {},
   "source": [
    "## Convert to CNF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "d31ee593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original ∀x∃y(p(x,y))\n",
      "eliminate_implication: ∀x∃y(p(x,y))\n",
      "move negation: ∀x∃y(p(x,y))\n",
      "remove double negation: ∀x∃y(p(x,y))\n",
      "standardize: ∀x∃a(p(x,a))\n",
      "move quan ∀x∃a(p(x,a))\n",
      "skolomize: ∀x (p(x,f(x)))\n",
      "elemniate universal (p(x,f(x)))\n",
      "CNF result: (p(x,f(x)))\n"
     ]
    }
   ],
   "source": [
    "def convert_to_cnf(sentence):\n",
    "    # Step 1: Eliminate implications\n",
    "    sentence = eliminate_implication(sentence)\n",
    "    print(\"eliminate_implication:\",sentence)\n",
    "    # Step 2: Move negation inward\n",
    "    sentence = inward_negation(sentence)\n",
    "    print(\"move negation:\",sentence)\n",
    "    # Step 3: Remove double negation\n",
    "    sentence = remove_double_negation(sentence)\n",
    "    print(\"remove double negation:\",sentence)\n",
    "    # Step 4: Standardize variable scope\n",
    "    sentence = standardize(sentence)\n",
    "    print(\"standardize:\",sentence)\n",
    "    # Step 5: Move quantifiers\n",
    "    sentence = move_quantifiers(sentence)\n",
    "    print(\"move quan\",sentence)\n",
    "    # Step 6: Skolemization\n",
    "    sentence = skolem(sentence)\n",
    "    print(\"skolomize:\",sentence)\n",
    "\n",
    "    # Step 7: Eliminate universal quantifiers\n",
    "    sentence = eliminate_universal_quantifiers(sentence)\n",
    "    print(\"elemniate universal\",sentence)\n",
    "    # Join clauses back into CNF sentence\n",
    "    cnf_sentence = sentence\n",
    "\n",
    "    return cnf_sentence\n",
    "\n",
    "# Define the test sentence\n",
    "test_sentence = \"∀x∃y(p(x,y))\"\n",
    "print(\"original\" , test_sentence)\n",
    "# Call the function and print the result\n",
    "cnf_result = convert_to_cnf(test_sentence)\n",
    "print(\"CNF result:\", cnf_result)\n",
    "\n",
    "################################# Test Cases#############################################\n",
    "\n",
    "#∀x(p(x)->q(x))\n",
    "#∀x∃y(p(x,y))\n",
    "#∃x∀y(¬(p(x,y)))\n",
    "#∃x(p(x))∧(q(b))\n",
    "# ¬¬(p(a))\n",
    "# ∀x∀y∀z(f(x,y)->f(y,z))∧(f(x,z))\n",
    "# ∀x(∃y(p(y,d,b,c))∨q(d,b))∧p(x,y)->r(x)\n",
    "# ∀x∃y∃z((l(x,y)∧l(y,z))∧(r(z)->p(z))∧(p(z)->r(z)))\n",
    "# ∀x((¬(p(a))∧(q(a))))\n",
    "# ∀x∀z∀u∀w((p(x,f(x),z))∨(p(u,w,w)))\n",
    "# ∀x∀a∀b(¬(p(x,a,b))∨¬(p(b,b,b)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "ed08356a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def turn_conjunctions_into_clauses(sentence):\n",
    "    # Split the sentence into clauses at each 'and'\n",
    "    clauses = sentence.split('∧')\n",
    "    return clauses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "e89a7e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_variables(sentence):\n",
    "    # Initialize a dictionary to store mappings of original variable names to unique variable names\n",
    "    variable_map = {}\n",
    "    counter = 0\n",
    "\n",
    "    # Ensure the input is a string by joining the clauses with ' ∨ ' delimiter\n",
    "    sentence = ' ∨ '.join(sentence)\n",
    "\n",
    "    # Tokenize the sentence into clauses\n",
    "    clauses = sentence.split(' ∨ ')\n",
    "\n",
    "    # Iterate through each clause\n",
    "    for i in range(len(clauses)):\n",
    "        clause = clauses[i]\n",
    "        # Tokenize the clause into words\n",
    "        tokens = clause.split()\n",
    "        # Iterate through the tokens\n",
    "        for j in range(len(tokens)):\n",
    "            token = tokens[j]\n",
    "            # Check if the token is a variable\n",
    "            if token.isalpha() and len(token) == 1:\n",
    "                # If the variable is already mapped, replace it with the mapped variable name\n",
    "                if token in variable_map:\n",
    "                    tokens[j] = variable_map[token]\n",
    "                else:\n",
    "                    # If not mapped, generate a new unique variable name and map it\n",
    "                    counter += 1\n",
    "                    new_variable_name = 'Var' + str(counter)\n",
    "                    variable_map[token] = new_variable_name\n",
    "                    # Replace the variable with the new variable name\n",
    "                    tokens[j] = new_variable_name\n",
    "        # Update the clause with the renamed variables\n",
    "        clauses[i] = ' '.join(tokens)\n",
    "\n",
    "    # Reconstruct the sentence from the modified clauses\n",
    "    renamed_sentence = ' ∨ '.join(clauses)\n",
    "\n",
    "    return renamed_sentence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c49d2e7",
   "metadata": {},
   "source": [
    "## Resolution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "0df5252a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resolve(clauses):\n",
    "    new_clauses = list(clauses)  # Create a copy of the original clauses\n",
    "    while True:\n",
    "        new_clause_count = len(new_clauses)\n",
    "        for i in range(new_clause_count):\n",
    "            for j in range(i + 1, new_clause_count):\n",
    "                clause1 = new_clauses[i]\n",
    "                clause2 = new_clauses[j]\n",
    "                resolvents = resolve_clause_pair(clause1, clause2)\n",
    "                if [] in resolvents:\n",
    "                    return False  # Empty clause indicates inconsistency\n",
    "                for resolvent in resolvents:\n",
    "                    if resolvent not in new_clauses:\n",
    "                        new_clauses.append(resolvent)\n",
    "        if len(new_clauses) == new_clause_count:\n",
    "            break  # No new clauses were added in this iteration\n",
    "    # If no empty clause is derived, the knowledge base is consistent\n",
    "    return True\n",
    "\n",
    "def resolve_clause_pair(clause1, clause2):\n",
    "    resolvents = []\n",
    "    for literal1 in clause1:\n",
    "        for literal2 in clause2:\n",
    "            if literal1 == negate_literal(literal2):\n",
    "                resolvent = resolve_literals(clause1, clause2, literal1, literal2)\n",
    "                if resolvent:\n",
    "                    resolvents.extend(resolvent)\n",
    "    return resolvents\n",
    "\n",
    "def negate_literal(literal):\n",
    "    if literal.startswith('¬'):\n",
    "        return literal[1:]\n",
    "    else:\n",
    "        return '¬' + literal\n",
    "\n",
    "\n",
    "def resolve_literals(clause1, clause2, literal1, literal2):\n",
    "    new_clause1 = [lit for lit in clause1 if lit != literal1]\n",
    "    new_clause2 = [lit for lit in clause2 if lit != literal2]\n",
    "    resolvent = new_clause1 + new_clause2\n",
    "    if not resolvent:\n",
    "        return [[]]  # Empty clause indicates inconsistency\n",
    "    return [resolvent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "7874ff4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eliminate_implication: ∀x(¬p(x) ∨ q(x))\n",
      "move negation: ∀x(¬p(x) ∨ q(x))\n",
      "remove double negation: ∀x(¬p(x) ∨ q(x))\n",
      "standardize: ∀x(¬p(x) ∨ q(x))\n",
      "move quan ∀x(¬p(x) ∨ q(x))\n",
      "skolomize: ∀x (¬p(x) ∨ q(x))\n",
      "elemniate universal (¬p(x) ∨ q(x))\n",
      "#################################\n",
      "eliminate_implication: (p(f,a))\n",
      "move negation: (p(f,a))\n",
      "remove double negation: (p(f,a))\n",
      "standardize: (p(f,a))\n",
      "move quan (p(f,a))\n",
      "skolomize: (p(f,a))\n",
      "elemniate universal (p(f,a))\n",
      "The knowledge base is inconsistent.\n"
     ]
    }
   ],
   "source": [
    "# Define your two sentences\n",
    "sentence1 = \"∀x(p(x)->q(x))\"\n",
    "sentence2 = \"(p(f,a))\"\n",
    "\n",
    "\n",
    "# Convert each sentence to CNF\n",
    "cnf_sentence1 = convert_to_cnf(sentence1)\n",
    "print(\"#################################\")\n",
    "cnf_sentence2 = convert_to_cnf(sentence2)\n",
    "\n",
    "# Separate the conjunctions into clauses for each CNF sentence\n",
    "clauses_sentence1 = turn_conjunctions_into_clauses(cnf_sentence1)\n",
    "clauses_sentence2 = turn_conjunctions_into_clauses(cnf_sentence2)\n",
    "\n",
    "# Combine the clauses from both sentences\n",
    "all_clauses = clauses_sentence1 + clauses_sentence2\n",
    "\n",
    "# Perform resolution\n",
    "is_inconsistent = resolve(all_clauses)\n",
    "\n",
    "if is_inconsistent:\n",
    "    print(\"The knowledge base is inconsistent.\")\n",
    "else:\n",
    "    print(\"The knowledge base is consistent.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
